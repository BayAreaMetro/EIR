"""
***************   ATTENTION: DO NOT EDIT THIS FILE    ***************

THESE FUNCTIONS ARE USED IN PRODUCTION SO ANY EDITS WILL LIKELY BREAK
EXISTING PROCESSES. 


Kaya Tollas is the only authorized editor of this file. If you have a
request or proposed edit, please notify her at ktollas@bayareametro.gov

Also please reach out if you have any questions about using these
functions.


This file contains commonly-used IO functions


Import with:

import sys
import getpass

user = getpass.getuser()
sys.dont_write_bytecode = True

# Select one of the following paths:

# # for DataViz team members
# sys.path.insert(0, '/Users/{}/Box/DataViz Projects/Utility Code'.format(user))

# # for all other teams with access to the Utility Code folder
# sys.path.insert(0, '/Users/{}/Box/Utility Code'.format(user))

from utils_io import *

"""


import io
import os
import sys
import glob
import json
import math
import time
import shutil
import getpass
import logging
import subprocess
import urllib.parse
import numpy as np
import pandas as pd
from datetime import datetime


# Pre-requisite: ! pip install boto3
try:
    import boto3
except:
    print('need to install boto3: pip install boto3')
# Pre-requisite: ! conda install -y -c anaconda psycopg2
# or ! pip install psycopg2-binary
try:
    import psycopg2
except:
    print('need to install psycopg2: pip install psycopg2 or conda install -y -c anaconda psycopg2')


# local imports
from utils_analytics import *
from teams import TEAMS

user = getpass.getuser()
sys.path.insert(0, '/Users/{}/Documents/GitHub/EIR/notebooks'.format(user))
try:
    from creds import *
except:
    print('creds not set, utils functionality will be limited')


def reload_module(modulename):
    """
    Use this to reload an updated module
    (to avoid needing to restart the kernel)
    
    Example usage:

    from utils_io import *  # initial imports
    reload_module('utils_io')  # reload after update to utils_io.py
    from utils_io import *  # reload reflects updates
    
    Source: https://stackoverflow.com/a/22073792
    """
    import importlib
    importlib.reload(sys.modules[modulename])


def run_bash_cmd(bash_cmd, logger=None):
    # result = subprocess.run(bash_cmd.split(), capture_output=True)
    result = subprocess.run(bash_cmd, capture_output=True, shell=True)
    log_or_print(result.stdout.decode('utf-8'), logger=logger)
    log_or_print(result.stderr.decode('utf-8'), logger=logger)


def init_logger(logger_name, output_dir='./'):
    """
    Given a logger_name and output_dir, sets the logging level to
    INFO and initializes logger to log to output_dir/<logger_name>.log
    and the console


    Parameters
    ----------
    logger_name : string
    output_dir : string


    Returns
    -------
    logger : logging.Logger
        Returns a logger with the given name and output configuration
    """
    logger = logging.getLogger(logger_name)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    log_fname = os.path.join(output_dir, '{}.log'.format(logger_name))
    fileHandler = logging.FileHandler(log_fname)
    logger.addHandler(fileHandler)
    consoleHandler = logging.StreamHandler()
    logger.addHandler(consoleHandler)
    logger.setLevel(logging.INFO)
    return logger


def log_or_print(print_str, logger=None):
    if logger is None:
        print(print_str)
    else:
        logger.info(print_str)


def read_text(fname):
    """
    Given a filename, returns the text in the file


    Parameters
    ----------
    fname : string


    Returns
    -------
    data : string
        Returns the file contents in a string
    """
    with open(fname) as f:
        data = f.read()
    return data


def write_text(text, fname, overwrite=True):
    """
    Given text and a filename, creates a file containing the text.
    If overwrite is False, appends to the file.

    Parameters
    ----------
    text : string
    fname : string
    overwrite: Boolean
    """
    if overwrite:
        open_arg = 'w' 
    else:
        open_arg = 'a'

    with open(fname, open_arg) as f:
        f.write(text)


def read_lines(fname=None, text_blob=None, split_delim='\n'):
    """
    Given a text blob or filename with data in lines, loads the lines as a list. Can optionally
    specify the split delimiter to split the file by something other than a newline


    Parameters
    ----------
    fname : string
    split_delim : string


    Returns
    -------
    lines : list
        Returns the file contents split by `split_delim` into a list of strings
    """
    no_info = fname is None and text_blob is None
    conflict = fname is not None and text_blob is not None
    if conflict or no_info:
        print('Need to provide either a text blob or filename')
        return
    if fname is not None:
        data = read_text(fname)
    if text_blob is not None:
        data = text_blob
    lines = data.split(split_delim)
    return [l for l in lines if len(str(l)) > 0]


def write_lines(lines, fname):
    """
    Given an iterable and filename, writes each item as a line to the file


    Parameters
    ----------
    lines : iterable
    fname : string
    """
    with open(fname, 'w') as f:
        for l in lines[:-1]:
            f.write(str(l) + '\n')
        f.write(str(lines[-1]))


def load_json(fname):
    """
    Given a filename, loads the content as a json object


    Parameters
    ----------
    fname : string


    Returns
    -------
    obj : dictionary
        Returns the file contents as a JSON dictionary
    """
    with open(fname) as f:
        obj = json.load(f)
    return obj


def dump_json(obj, fname):
    """
    Given a json object and a filename, writes the object to the file


    Parameters
    ----------
    obj: JSON-compatible object (e.g. dictionary, list)
    fname : string
    """
    with open(fname, 'w') as f:
        json.dump(obj, f, indent=4)


def read_sql_cmds(sql_file):
    sql_cmds = read_text(sql_file)
    sql_cmds = sql_cmds.split(';')[:-1]
    sql_cmds = [c + ';' for c in sql_cmds]
    final_cmds = []
    # remove commands that are fully commented out
    for cmd in sql_cmds:
        lines = read_lines(text_blob=cmd)
        if not np.array([l.startswith('--') for l in lines]).all():
            final_cmds.append(cmd)  # this query is commented out
    return final_cmds


def delete_file(fname):
    """
    Given a filename, deletes it if it exists
    """
    if os.path.isfile(fname):
        os.remove(fname)


def delete_dir(dirname):
    """
    Given a directory name, deletes it and all its contents if it exists
    """
    if os.path.exists(dirname):
        shutil.rmtree(dirname)


def makedirs_if_not_exists(dirname):
    """
    Given a directory name, creates the directory if it does not exist.
    If it does exist, prints warning message


    Parameters
    ----------
    dirname : string
    """
    if os.path.exists(dirname):
        overwrite_warning = 'Files in this directory may be overwritten'
        print('Warning: directory exists: {} \n{}'.format(dirname, overwrite_warning))
    else:
        os.makedirs(dirname)


def create_zip(dirname, output_filename=None):
    """
    Given a directory name and optionally an output_filename, creates a
    zipfile from the directory named {output_filename}.zip (if given,
    otherwise output_filename is the basename of the directory).
    """
    if output_filename is None:
        output_filename = os.path.basename(dirname)
    shutil.make_archive(output_filename, 'zip', dirname)


def extract_zip(input_zip):
    """
    Extracts a zipfile into memory

    Source: https://stackoverflow.com/a/10909016
    """
    from zipfile import ZipFile
    input_zip = ZipFile(input_zip)
    return {name: input_zip.read(name) for name in input_zip.namelist()}


def unzip_file(zip_fname, output_dirname=None):
    """
    Unzips a .zip/.gz/.tar.gz file to a given output_dirname
    or its filename in the working directory if none is provided.
    """
    fname_base = os.path.basename(zip_fname)
    fname_components = os.path.splitext(fname_base)
    if output_dirname is None:
        output_dirname = '.'
    if fname_components[1] == '.gz':
        if os.path.splitext(fname_components[0])[1] != '.tar':
            import gzip
            with gzip.open(zip_fname, 'rb') as f_in:
                with open(output_dirname, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
        else:  # tar
            import tarfile
            with tarfile.open(zip_fname) as tar:
                tar.extractall(path=output_dirname)
    if fname_components[1] == '.zip':
        from zipfile import ZipFile
        with ZipFile(zip_fname, 'r') as zip_ref:
            zip_ref.extractall(output_dirname)
    print('unzipped file at {}'.format(output_dirname))


def find_files(pattern):
    """
    Uses glob to return a list of files matching the path pattern
    
    Examples:
    
    Matching files in a folder:
    
        files = find_files('~/Downloads/file_*.csv')
    
    Matching files in a folder and its subfolders:
    
        files = find_files('~/Downloads/hello_world/**/*.json')
        
    Glob docs: https://docs.python.org/3/library/glob.html
    """
    recursive = False
    if '**' in pattern:
        recursive = True
    return glob.glob(pattern, recursive=recursive)


def df_to_markdown_table(df, markdown_table_fname, center=False, bold_cols=None):
    df = df.astype(str)
    header_row = '**{}**'.format('**|**'.join(list(df)))
    if center:
        sep = '|'.join([':-----:'] * len(list(df)))
    else:
        # left-aligned
        sep = '|'.join(['-----'] * len(list(df)))

    md_str = '{}\n{}\n'.format(header_row, sep)
    for idx, row in df.iterrows():
        row_vals = []
        for idx, val in row.items():
            if not is_null(val):
                if bold_cols is not None and idx in bold_cols:
                    row_vals.append('**{}**'.format(val))
                else:
                    row_vals.append(val)
            else:
                row_vals.append(' ')
        md_str += '|'.join(row_vals) + '\n'
    md_str = md_str.replace('_', '\_')

    write_text(md_str, markdown_table_fname)


def write_sql_cmds(cmds, fname='sql_commands.sql', overwrite=True):
    """
    Given a list of SQL command strings, writes them to the given fname


    Parameters
    ----------
    logger_name : string
    output_dir : string
    """
    sql_commands = '\n\n'.join(cmds)
    write_text(sql_commands, fname, overwrite=overwrite)


def set_AWS_creds():
    """
    This function sets AWS keys in the environment in order to connect to AWS
    via boto3. This function is mostly used internally by functions that connect
    to AWS. It reads keys from Box: DataViz Projects/Application Secure Files/creds.py
    """
    creds_to_set = ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY']
    global AWS_CREDS
    other_teams = {k: v for k, v in TEAMS.items() if k != 'dataviz_team'}

    if user in TEAMS['dataviz_team']:
        try:
            if user in AWS_CREDS:
                for key in creds_to_set:
                    os.environ[key] = AWS_CREDS[user][key]
        except:
            print('Using DataViz team creds')
            for key in creds_to_set:
                os.environ[key] = AWS_CREDS['dataviz_team'][key]

    elif user in itertools.chain(*[v['members'] for v in other_teams.values()]):
        user_team = [k for k, v in other_teams.items() if user in v['members']][0]
        creds_folder = TEAMS[user_team]['folder']
        print('Using {}'.format(creds_folder))
        sys.path.insert(0, '/Users/{}/Box/{}'.format(user, creds_folder))
        from creds import AWS_CREDS
        for key in creds_to_set:
            os.environ[key] = AWS_CREDS[user_team][key]

    else:
        set_creds_msg = """Your credentials for programmatic AWS access have not been set.
        Please go to Box: DataViz Projects/Application Secure Files and add your credentials
        to the AWS_CREDS section in creds.py"""
        print(set_creds_msg)


def get_AWS_creds():
    AWS_ACCESS_KEY_ID = os.environ['AWS_ACCESS_KEY_ID']
    AWS_SECRET_ACCESS_KEY = os.environ['AWS_SECRET_ACCESS_KEY']
    return AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY


def list_s3_buckets():
    """
    From https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-creating-buckets.html#list-existing-buckets  # noqa
    """
    set_AWS_creds()
    # Retrieve the list of existing buckets
    s3 = boto3.client('s3')
    response = s3.list_buckets()
    return [bucket['Name'] for bucket in response['Buckets']]


def list_s3_keys(bucket, **kwargs):
    """
    Given the name of a bucket on S3 and optionally additional filter arguments,
    returns a list of all matching filenames

    Filter arguments: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Bucket.objects  # noqa
    

    Parameters
    ----------
    bucket: string
    **kwargs: optional, can specify Delimiter, EncodingType, Marker, MaxKeys, Prefix

    Returns
    -------
    files : list
        Returns a list of S3 files matching the specified filter conditions (if any)
    """
    set_AWS_creds()
    s3 = boto3.resource('s3')
    bucket = s3.Bucket(bucket)
    files = [key.key for key in bucket.objects.filter(**kwargs)]
    return files


def check_if_object_exists_on_s3(bucket, key):
    """
    From https://stackoverflow.com/a/34562141
    """
    s3 = boto3.resource('s3')
    s3_bucket = s3.Bucket(bucket)
    objs = list(s3_bucket.objects.filter(Prefix=key))
    if len(objs) > 0 and objs[0].key == key:
        return True
    else:
        return False


def delete_s3_keys(bucket, Prefix):
    """
    Given the name of a bucket on S3 and optionally additional filter arguments
    (same as those for `list_s3_keys`, except Prefix which must be specified),
    returns the delete request metadata

    
    Parameters
    ----------
    bucket: string
    Prefix: string

    Returns
    -------
    metadata : dictionary
        Returns a dictionary of delete request metadata
    """
    set_AWS_creds()
    s3 = boto3.resource('s3')
    bucket = s3.Bucket(bucket)
    objects = bucket.objects.filter(Prefix=Prefix)
    return objects.delete()


def copy_s3_key(source_bucket, source_key, dest_bucket, dest_key):
    """
    Given the name of a source bucket and key on S3 and a destination bucket/key,
    copies the source key to the destination bucket/key

    
    Parameters
    ----------
    source_bucket: string
    source_key: string
    dest_bucket: string
    dest_key: string
    """
    set_AWS_creds()
    s3 = boto3.resource('s3')
    copy_source = {'Bucket': source_bucket,
                   'Key': source_key}

    bucket = s3.Bucket(dest_bucket)
    bucket.copy(copy_source, dest_key)


def rename_s3_key(orig_bucket, orig_key, new_bucket, new_key, logger=None):
    """
    Given the name of the original bucket and key on S3 and a new bucket/key,
    moves the original key to the new bucket + key

    https://docs.aws.amazon.com/cli/latest/reference/s3/mv.html

    
    Parameters
    ----------
    orig_bucket: string
    orig_key: string
    new_bucket: string
    new_key: string
    """
    # copy_s3_key(orig_bucket, orig_key, new_bucket, new_key)
    # delete_s3_keys(orig_bucket, orig_key)
    # print_str = 'renamed s3://{}/{} to s3://{}/{}'.format(orig_bucket, orig_key, new_bucket, new_key)
    # log_or_print(print_str, logger)
    s3_mv_cmd = ('aws s3 mv s3://{}/{} s3://{}/{}'
             .format(orig_bucket,
                     orig_key,
                     new_bucket,
                     new_key)
             .split())

    result = subprocess.run(s3_mv_cmd, capture_output=True)
    log_or_print(result.stdout.decode('utf-8'))
    log_or_print(result.stderr.decode('utf-8'))


def copy_file_to_s3(local_fname, bucket, key):
    """
    Given a local filename, an S3 bucket, and destination S3 key, copies the file
    to the S3 destination
    """
    set_AWS_creds()
    s3 = boto3.resource('s3')
    s3.Bucket(bucket).upload_file(local_fname, key)
    # cp_cmd = 'aws s3 cp {} s3://{}/{}'.format(local_fname, bucket, key)
    # cp_cmd = "aws s3 cp '{}' 's3://{}/{}'".format(local_fname, bucket, key)
    # run_bash_cmd(cp_cmd)


def download_file_from_s3(bucket, key, local_fname):
    set_AWS_creds()
    s3 = boto3.client('s3')
    s3.download_file(bucket, key, local_fname)


# def download_df_from_s3(bucket, key, logger=None):
#     """
#     Given an S3 bucket and filename of a table on S3, downloads the
#     file as a pandas DataFrame
#     """
#     set_AWS_creds()
#     s3 = boto3.client('s3')
#     print_str = 'downloading S3 data from {}:{}'.format(bucket, key)
#     log_or_print(print_str, logger)
#     obj = s3.get_object(Bucket=bucket, Key=key)
#     df = pd.read_csv(io.BytesIO(obj['Body'].read()))
#     return df


def pull_df_from_s3(bucket, key, logger=None, **kwargs):
    """
    Given an S3 bucket and filename of a table on S3, downloads the
    file as a pandas DataFrame. Replacement function for download_df_from_s3

    Requires AWS CLI to be installed-- for Mac users, use brew install awscli
    """
    temp_fname = '{}.csv'.format(generate_uuid_list(1)[0])
    s3_cp_cmd = ('aws s3 cp s3://{}/{} {}'.format(bucket, key, temp_fname)).split()
    result = subprocess.run(s3_cp_cmd, capture_output=True)
    log_or_print(result.stdout.decode('utf-8'), logger=logger)
    log_or_print(result.stderr.decode('utf-8'), logger=logger)
    try:
        df = pd.read_csv(temp_fname, **kwargs)
    except Exception as e:
        print('Failed to load DataFrame from csv')
        print(e)
    os.remove(temp_fname)
    return df


def post_df_to_s3(df, bucket, key, logger=None):
    """
    Given a pandas DataFrame, S3 bucket name, and S3 dest filename, 
    posts a dataframe as a csv in S3.

    This function is from https://stackoverflow.com/a/40615630
    """
    set_AWS_creds()
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False)
    s3_resource = boto3.resource('s3')
    s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())
    print_str = 'dataframe on S3 at {}:{}'.format(bucket, key)
    log_or_print(print_str, logger)


def post_df_to_s3_alt(df, bucket, key):
    """
    This function is from https://stackoverflow.com/a/53725639
    and is meant to overcome the memory problems from the above
    post_df_to_s3 function.

    Disclaimer: this took about the same amount of time (forever)
    as the above function on a large dataframe 
    """
    # Pre-requisite: ! pip install s3fs
    import s3fs
    set_AWS_creds()
    s3 = s3fs.S3FileSystem()
    # Use 'w' for py3, 'wb' for py2
    with s3.open('{}/{}'.format(bucket, key), 'w') as f:
        df.to_csv(f)


def get_redshift_creds(db_user='admin'):
    global REDSHIFT_CREDS
    other_teams = {k: v for k, v in TEAMS.items() if k != 'dataviz_team'}

    if user in TEAMS['dataviz_team']:
        user_creds = REDSHIFT_CREDS[db_user]
        redshift_creds = {'user': user_creds['username'],
                          'password': user_creds['password'],
                          'host': REDSHIFT_CREDS['host']}

    elif user in itertools.chain(*[v['members'] for v in other_teams.values()]):
        user_team = [k for k, v in other_teams.items() if user in v['members']][0]
        creds_folder = TEAMS[user_team]['folder']
        print('Using {}'.format(creds_folder))
        sys.path.insert(0, '/Users/{}/Box/{}'.format(user, creds_folder))
        from creds import REDSHIFT_CREDS
        user_creds = REDSHIFT_CREDS[db_user]
        redshift_creds = {'user': user_creds['username'],
                          'password': user_creds['password'],
                          'host': REDSHIFT_CREDS['host']}

    else:
        set_creds_msg = """Your credentials for Redshift access have not been set.
        Please reach out to the DataViz team via Slack to establish/register your credentials"""
        print(set_creds_msg)
    return redshift_creds


def create_redshift_connection(dbname='dev'):
    """
    Returns a psycopg2 Redshift connection to the given dbname
    """
    redshift_creds = get_redshift_creds()
    db_connection_params = {'port': 5439,  
                            'dbname': dbname}
    redshift_connection_params = {**redshift_creds, **db_connection_params}

    # system maintenance currently scheduled for every Wednesday between 12:00-12:30pm UTC
    update_weekday = 2
    update_hour = 12
    update_start_min = 0
    update_end_min = 30

    now = datetime.utcnow()
    if (now.weekday() == update_weekday
        and now.hour == update_hour
        and now.minute >= update_start_min
        and now.minute <= update_end_min):
        print('Cluster is currently scheduled for system maintenance is likely offline.')
        sleep_time = update_end_min - now.minute
        print('Sleeping for the next {} minutes until maintenance is complete.'.format(sleep_time))
        time.sleep(sleep_time * 60)
        print('Done sleeping, retrying Redshift connection.')

    conn = psycopg2.connect(**redshift_connection_params)
    return conn


def get_geoserver_creds():
    user_creds = GEOSERVER_CREDS[user]
    geoserver_creds = {'user': user_creds['username'],
                       'password': user_creds['password'],
                       'database': user_creds['database'],
                       'host': GEOSERVER_CREDS['host']}
    return geoserver_creds


def create_geoserver_connection():
    conn_args = get_geoserver_creds()
    conn = psycopg2.connect(**conn_args)
    return conn


def create_redshift_engine(dbname='dev'):
    """
    Creates a default Redshift SQLAlchemy engine for pandas-Redshift connections

    This connection is only used for small data that doesn't need to rely
    on S3. Medium to large data updates take way too long with this method.
    """
    # Pre-requisite: ! pip install sqlalchemy-redshift
    try:
        from sqlalchemy import create_engine
    except:
        print('need to install sqlalchemy: pip install sqlalchemy-redshift')
    redshift_creds = get_redshift_creds()
    db_connection_params = {'port': 5439,  
                            'dbname': dbname}
    redshift_connection_params = {**redshift_creds, **db_connection_params}

    str_base = 'redshift+psycopg2://{}:{}@{}:{}/{}'
    psql_engine_str = str_base.format(redshift_connection_params['user'],
        redshift_connection_params['password'],
        redshift_connection_params['host'],
        redshift_connection_params['port'],
        redshift_connection_params['dbname'])

    engine = create_engine(psql_engine_str, echo=True)
    return engine


def execute_redshift_cmds(cmd_list, dbname='dev',
    geoserver=False, autocommit=False, print_messages=False,
    logger=None):
    """
    Given a list of SQL strings, executes them in order on Redshift
    """
    if geoserver:
        conn = create_geoserver_connection()
    else:  # regular redshift connection
        conn = create_redshift_connection(dbname)
    if autocommit:
        conn.set_session(autocommit=autocommit)
    cur = conn.cursor()
    for cmd in cmd_list:
        # print command
        # scrub AWS creds from Redshift-S3 commands, like COPY and UNLOAD
        if cmd.startswith('COPY') or cmd.startswith('UNLOAD'):
            scrub_cmd = scrub_creds_from_aws_cmd(cmd)
            log_or_print(scrub_cmd + '\n\n', logger)
        else:
            log_or_print(cmd + '\n\n', logger)
        # execute command
        cur.execute(cmd)
        if print_messages:
            print(cur.statusmessage)
            # # only for SELECT, INSERT, UPDATE, and DELETE
            # # (https://stackoverflow.com/a/24441715)
            # print('number of rows affected: {}'.format(cur.rowcount))
    conn.commit()
    cur.close()
    conn.close()


def list_all_schema_tables_redshift(schema=None, dbname='dev', geoserver=False):
    if schema is not None:
        list_tables_cmd = """SELECT t.table_name
                            FROM information_schema.tables AS t
                            WHERE t.table_schema = '{}'
                            ORDER BY 1;""".format(schema)
    else:  # pull tables from all schemas
        list_tables_cmd = """SELECT t.table_schema, t.table_name
                            FROM information_schema.tables AS t
                            ORDER BY 1;""".format(schema)
    res = pull_df_from_redshift_sql(list_tables_cmd,
        dbname=dbname,
        geoserver=geoserver,
        print=False)
    if schema is not None:
        return list(res['table_name'])
    else:
        return res


def check_if_table_exists_on_redshift(tablename, dbname='dev', geoserver=False):
    select_all_rs_tables_q = """SELECT t.table_schema, t.table_name
                                FROM information_schema.tables t"""
    rs_tables = pull_df_from_redshift_sql(select_all_rs_tables_q,
        dbname=dbname,
        geoserver=geoserver,
        print=False)
    schema = None
    if len(tablename.split('.')) > 1:
        schema = tablename.split('.')[0]
        tablename = tablename.split('.')[1]

    tables = rs_tables[rs_tables['table_name'] == tablename]
    if schema is not None:
        tables = tables[tables['table_schema'] == schema]
    if len(tables) == 0:
        return False
    else:
        return True


def get_nrows_redshift(tablename, dbname='dev', geoserver=False):
    """
    Gets the number of rows of the table of Redshift.

    From https://dataedo.com/kb/query/amazon-redshift/list-of-tables-by-the-number-of-rows  # noqa
    """
    nrows_q = """SELECT tinf.tbl_rows
    FROM svv_tables tab
    JOIN svv_table_info tinf
    ON tab.table_schema = tinf.schema
    AND tab.table_name = tinf.table
    WHERE tab.table_schema='{}' AND tab.table_name='{}';""".format(*(tablename.split('.')))

    pull_from_rs_args = {'sql_statement': nrows_q,
                         'dbname': dbname,
                         'geoserver': geoserver,
                         'print': False}
    nrows = pull_df_from_redshift_sql(**pull_from_rs_args)
    if len(nrows) > 0:
        return int(nrows['tbl_rows'][0])


def get_ctypes_redshift(tablename, dbname='dev', geoserver=False):
    """
    Gets the column types of the table on Redshift.
    WARNING: these are not the same ctypes needed to initialize a table
    These are: https://docs.aws.amazon.com/redshift/latest/dg/c_Supported_data_types.html
    """
    # pull Redshift table metadata
    schema, table = tablename.split('.')
    set_path_sql = 'set search_path to {}'.format(schema)
    coltype_sql = """SELECT \"column\", type
    FROM pg_table_def WHERE tablename = '{}'""".format(table)
    ctypes_cmd = set_path_sql + ';' + coltype_sql
    coltype_df = pull_df_from_redshift_sql(ctypes_cmd,
        dbname=dbname,
        geoserver=geoserver,
        print=False)
    ctypes = dict(zip(coltype_df['column'], coltype_df['type']))
    # text replace for 'character varying' -> varchar
    for k, v in ctypes.items():
        if 'character varying' in v:
            ctypes[k] = v.replace('character varying', 'varchar')
        if v == 'timestamp without time zone':
            ctypes[k] = 'timestamp'
        if v == 'timestamp with time zone':
            ctypes[k] = 'timestamptz'
        if v == 'double precision':
            ctypes[k] = 'float'
    return ctypes


def get_geom_cols_redshift(tablename, **kwargs):
    """
    Given a Redshift tablename, returns a list of the geometric columns
    """
    ctypes = get_ctypes_redshift(tablename, **kwargs)
    geom_cols = [c for c in ctypes if ctypes[c] == 'geometry']
    return geom_cols


def get_max_field_len_redshift(fieldname, tablename, dbname='dev'):
    max_len_str = 'select max(len({})) from {}'.format(fieldname, tablename)
    df = pull_df_from_redshift_sql(max_len_str, dbname=dbname, print=False)
    return df['max'][0]


def get_compressed_ctypes(tablename, dbname='dev', compression_pct=0.5):
    """
    Given a Redshift tablename and optionally a
    compression percentage (a float between 0 and 1, inclusive,
    where 0 corresponds to 0 possible compression and 1 corresponds to
    full compression to the largest observed character length),
    compresses the character fields.              
    """
    ctypes = get_ctypes_redshift(tablename, dbname)
    for k, v in ctypes.items():
        if 'char' in v:
            # get max observed string field length
            max_len  = get_max_field_len_redshift(k, tablename, dbname)
            # extract specified string field length from char/varchar ctypes
            current_field_len = int(v.split('(')[1].strip(')'))
            # set new field length
            new_field_len = max_len + int((current_field_len - max_len) * (1- compression_pct))
            new_field_type = '{}({})'.format(v.split('(')[0],
                                             new_field_len)
            print('reducing {} from {} to {}'.format(k, v, new_field_type))
            ctypes[k] = new_field_type
    return ctypes


def get_table_keys_redshift(tablename, dbname='dev', geoserver=False):
    """
    Returns the metadata for distkey and sortkey columns in a Redshift table
    """
    schema, table = tablename.split('.')

    set_path_sql = "set search_path to '$user', 'public', '{}'".format(schema)
    keys_sql = """SELECT * FROM pg_table_def
                WHERE tablename = '{}'
                AND (distkey IS TRUE OR sortkey > 0);""".format(table)
    keys_cmd = set_path_sql + ';' + keys_sql
    keys_df = pull_df_from_redshift_sql(keys_cmd,
        dbname=dbname,
        geoserver=geoserver,
        print=False)
    return keys_df


def get_distkey(tablename, dbname='dev', geoserver=False):
    keys_df = get_table_keys_redshift(tablename, dbname, geoserver)
    distkey = keys_df['column'][keys_df['distkey'] == True]
    if len(distkey) == 1:
        return distkey.iloc[0]
    else:
        return None
    

def get_sortkey(tablename, dbname='dev', geoserver=False):
    keys_df = get_table_keys_redshift(tablename, dbname, geoserver)
    sortkey = keys_df['column'][keys_df['sortkey'][keys_df['sortkey'] > 0].sort_values().index]
    if len(sortkey) > 0:
        if len(sortkey) == 1:
            sortkey = sortkey.iloc[0]
        else:
            sortkey = list(sortkey.values)
    else:
        sortkey = None
    return sortkey


def pull_df_from_redshift_sql(sql_statement, dbname='dev', geoserver=False,
    logger=None, print=True):
    """
    Connects to the default Redshift engine and returns a
    dataframe based on the SQL statement

    Was originally:

    # def pull_df_from_redshift_sql(sql_statement, dbname='dev'):
    #     # Using SQLAlchemy, connects to the default Redshift
    #     # engine and returns a dataframe based on the SQL statement

    #     engine = create_redshift_engine(dbname)
    #     return pd.read_sql(sql_statement, engine)

    but psycopg2 is preferred.
    """
    a = time.time()
    if geoserver:
        conn = create_geoserver_connection()
    else:  # regular redshift connection
        conn = create_redshift_connection(dbname)
    cur = conn.cursor()
    cur.execute(sql_statement)
    data = cur.fetchall()
    cur.close()
    conn.close()
    cols = [c[0] for c in cur.description]
    b = time.time()
    final_df = pd.DataFrame(data, columns=cols)
    if print:
        print_str = 'took {}'.format(print_runtime(b-a))
        log_or_print(print_str, logger)
    return final_df


def pull_geotable_redshift(select_q, crs='EPSG:4326', **kwargs):
    import sql_metadata
    import geopandas as gpd
    table = sql_metadata.get_query_tables(select_q)[0]
    geom_cols = get_geom_cols_redshift(table, **kwargs)
    if len(geom_cols) < 1:
        print('Error: not a geo-table. Use pull_df_from_redshift_sql instead')
    main_geo_col = geom_cols[0]
    gdf = pull_df_from_redshift_sql(select_q, **kwargs)
    # convert Redshift-native WKB into geopandas Shapely object
    gdf[main_geo_col] = gdf[main_geo_col].map(lambda x: to_shapely(wkb_to_wkt(x)))
    gdf = gpd.GeoDataFrame(gdf,
                           geometry=main_geo_col,
                           crs=crs)
    return gdf


def pull_large_df_from_redshift_sql(sql_statement, dbname='dev', geoserver=False,
    chunksize=100000):
    """
    For queries that reutrn lots of rows, pulls them in chunks and returns
    a single dataframe
    """
    if geoserver:
        conn = create_geoserver_connection()
    else:  # regular redshift connection
        conn = create_redshift_connection(dbname)
    cur = conn.cursor()
    # get number of rows returned by query
    query_nrows = 'SELECT count(*) FROM ({})'.format(sql_statement)
    cur.execute(query_nrows)
    nrows = cur.fetchall()[0][0]
    tablename = sql_statement.split('FROM')[1].split()[0]
    q_tablename = tablename + '_query'
    select_q = sql_statement.split('FROM')[0]
    drop_q_table_str = 'DROP TABLE if exists {}'.format(q_tablename)
    create_q_table_str = """CREATE TABLE {} AS
     (SELECT *, ROW_NUMBER () OVER (ORDER BY 1)
      FROM {})""".format(q_tablename, tablename)
    execute_redshift_cmds([drop_q_table_str, create_q_table_str], dbname=dbname)
    
    num_chunks = math.ceil(nrows / chunksize)
    print('pulling {} rows in {} chunks of {}'.format(nrows,
        num_chunks,
        chunksize))
    dfs = []
    for chunk in range(num_chunks):
        print('pulling chunk {}'.format(chunk))
        new_q = """{}
                FROM {}
                ORDER BY row_number
                OFFSET {} LIMIT {};""".format(select_q,
                    q_tablename,
                    chunk*chunksize,
                    chunksize)
        dfs.append(pull_df_from_redshift_sql(new_q, dbname=dbname))
    
    execute_redshift_cmds([drop_q_table_str], dbname=dbname)
    return pd.concat(dfs)


def rename_table_redshift(old_schema, old_tablename, new_schema, new_tablename,
    old_dbname, new_dbname, geoserver=False):
    """
    Renames Redshift table (from one schema or DB to another)
    (based on https://stackoverflow.com/a/48915446)

    If renaming within same schema, use the basic SQL query
        ALTER TABLE <schema.old_tablename> RENAME TO <new_tablename>
    """
    copy_table_args = {'old_tablename': '{}.{}'.format(old_schema, old_tablename),
                       'old_dbname': old_dbname,
                       'new_tablename': '{}.{}'.format(new_schema, new_tablename),
                       'new_dbname': new_dbname,
                       'geoserver': geoserver}

    copy_table_redshift(**copy_table_args)
    cmd_list = ['DROP TABLE {}.{};'.format(old_schema,
        old_tablename)]
    execute_redshift_cmds(cmd_list, autocommit=True, dbname=old_dbname)


def create_dummy_redshift_dataset(use_max_len=False, use_distkey=True,
    use_sortkey=True):
    # Create dummy datasets
    df1 = create_dummy_dataset()
    df2 = create_dummy_dataset()
    df = pd.concat([df1, df2], ignore_index=True, sort=False)

    # Get Redshift column types
    ctypes = create_column_type_dict(df)
    if use_max_len:  # make char cols max length for testing purposes
        max_len = 'varchar(max)'
        for k, v in ctypes.items():
            if 'char' in v: 
                ctypes[k] = max_len

    # Post to S3
    bucket = 'dataviz-analytics'
    key = 'test_table.csv'

    post_df_to_s3(df, bucket, key)
    
    # Create on Redshift
    if use_distkey:
        distkey = 'd'
    else:
        distkey = None
    
    if use_sortkey:
        sortkey = ['b', 'e']
    else:
        sortkey = None

    tablename = 'basis.test_table'
    create_table_args = {'tablename': tablename,
                         's3_path': 's3://{}/{}'.format(bucket, key),
                         'ctypes': ctypes,
                         'distkey': distkey,
                         'sortkey': sortkey}
    create_redshift_table_via_s3(**create_table_args)


def copy_data_redshift(source_tablename, source_dbname, dest_tablename, dest_dbname,
                       select_str=None):
    """
    Given a source tablename/db and destination tablename/db, copies data from the source to the dest.

    Notes:
    ------
    - source_tablename and dest_tablename must exist on source_dbname and dest_dbname, respectively,
    and have matching column names (or the dest column names must match those specified in the select_str)

    """
    if select_str is None:
        select_str = 'SELECT *'
    
    if source_dbname != dest_dbname:
        bucket = 'dataviz-analytics'
        unload_data_args = {'bucket': bucket,
                            'tablename': source_tablename,
                            'dbname': source_dbname,
                            'select_q': select_str}
        key_prefix = unload_data_from_redshift_to_s3(**unload_data_args)
        s3_path = 's3://{}/{}'.format(bucket, key_prefix)

        copy_data_args = {'tablename': dest_tablename,
                          's3_path': s3_path,
                          'dbname': dest_dbname}
        copy_data_from_s3_to_redshift(**copy_data_args)
    else:
        insert_into_cmd = 'INSERT INTO {} ({} FROM {})'.format(dest_tablename,
                                                             select_str,
                                                             source_tablename)
        execute_redshift_cmds([insert_into_cmd], dbname=dest_dbname)


def copy_table_redshift(old_tablename, old_dbname, new_tablename,
    new_dbname, compression_pct=0, sortkey=None, distkey=None,
    select_cols=None, replace_if_exists=True, drop_cascade=False,
    geoserver=False, logger=None):
    """
    Given an old_tablename (of format old_schema.old_table),
    old_dbname, new_tablename (of format new_schema.new_table),
    and new_dbname, copies a Redshift table.

    Optionally you may specify compression_pct (float),
    sortkey (string or list of strings), distkey (string),
    select_cols (list of strings), and replace_if_exists (boolean).
    
    If compression_pct > 0, will create a table copy with the given
    character type compression (0 is no compression, 1 is full possible compression).
    Choose 1 if you are sure new data loads will always be within the maximum bounds
    of your current data (e.g. if a string field will never be longer than 40 characters).
    
    If the distkey and the sortkey are not specified, they will be the same as in old_tablename.
    To remove an existing distkey/sortkey on the new table, set to False

    You may specify select_cols, a list of the columns to select from the table.

    If replace_if_exists is False, the function will return
    with a 'table exists, not replacing' message.
    
    Example usage:
    --------------
    copy_table_args = {'old_tablename': 'basis.test_table_1',
                       'old_dbname': 'dev',
                       'new_tablename': 'basis.test_table',
                       'new_dbname': 'staging',
                       'compression_pct': 0.9,  # optional, default is 0
                       'select_cols': None,  # optional, default is None
                       'sortkey': None,  # optional, default is None
                       'distkey': None,  # optional, default is None
                       'geoserver': False,  # optional, default is False
                       'replace_if_exists': True,  # optional, default is True
                       'drop_cascade': True,  # optional, default is False
                       'logger': None}  # optional, default is None

    copy_table_redshift(**copy_table_args)

    
    Table copy rules applied:
    -------------------------
    - if the tables are on a different db, then UNLOAD + COPY (S3)
    - else:
        - if the table is the same (e.g. compression_pct=0, sortkey=sortkey, distkey=distkey), then CTAS
        - else: CREATE TABLE NEW and then INSERT INTO

    Same DB copy based on https://docs.aws.amazon.com/redshift/latest/dg/performing-a-deep-copy.html  # noqa

    Different DB copy based on https://stackoverflow.com/a/30069630
    """
    # First check if dest table exists
    on_rs = check_if_table_exists_on_redshift(new_tablename, new_dbname)    
    if on_rs:
        if replace_if_exists:
            print_str = 'replacing {} on Redshift'.format(new_tablename)
            log_or_print(print_str, logger)
            drop_table_cmd = 'DROP TABLE IF EXISTS {}'.format(new_tablename)
            if drop_cascade:
                drop_table_cmd += ' CASCADE'
            execute_redshift_cmds([drop_table_cmd], dbname=new_dbname)
        else:
            print_str = 'table exists, not replacing'
            log_or_print(print_str, logger)
            return
        
    # Determine distkey and sortkey
    # get existing table distkey and sortkey
    old_distkey = get_distkey(old_tablename, old_dbname)
    old_sortkey = get_sortkey(old_tablename, old_dbname)

    # if distkey/sortkey is None (default), uses the existing
    if distkey is None:
        distkey = old_distkey
    if sortkey is None:
        sortkey = old_sortkey
        
    # if distkey/sortkey is False, removes from the new table
    if distkey is False:
        distkey = None
    if sortkey is False:
        sortkey = None

    # Establish select cols
    if select_cols is None:
        select_cols = '*'
    else:
        select_cols = ', '.join(select_cols)

    # Establish select string
    select_str = 'SELECT {} FROM {}'.format(select_cols, old_tablename)

    # Same db copy
    if old_dbname == new_dbname:
        # If new table is different from existing table
        if compression_pct > 0 or distkey != old_distkey or sortkey != old_sortkey:
            # https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_TABLE_NEW.html                
            ctypes = get_compressed_ctypes(old_tablename, old_dbname, compression_pct)
            create_table_str_args = {'tablename': new_tablename,
                                     'ctypes': ctypes,
                                     'distkey': distkey,
                                     'sortkey': sortkey}
            create_table_cmd = create_redshift_table_str(**create_table_str_args)
            insert_into_cmd = 'INSERT INTO {} ({})'.format(new_tablename,
                                                           select_str)
            cmd_list = [create_table_cmd,
                        insert_into_cmd]

        # If the new table is identical to the existing table:
        # (i.e. 0 compression, same distkey and sortkey)
        else:
            create_table_cmd = 'CREATE TABLE {} AS {}'.format(new_tablename,
                                                              select_str)
            cmd_list = [create_table_cmd]

        execute_redshift_cmds(cmd_list, dbname=new_dbname, autocommit=True)
    # Different db copy
    else:
        bucket = 'dataviz-analytics'
        unload_data_args = {'bucket': bucket,
                            'tablename': old_tablename,
                            'dbname': old_dbname,
                            'select_q': select_str}
        key_prefix = unload_data_from_redshift_to_s3(**unload_data_args)
        s3_path = 's3://{}/{}'.format(bucket, key_prefix)
        if compression_pct > 0:
            ctypes = get_compressed_ctypes(old_tablename, old_dbname, compression_pct)
        else:
            ctypes = get_ctypes_redshift(old_tablename, old_dbname)
        create_table_args = {'tablename': new_tablename,
                             's3_path': s3_path,
                             'ctypes': ctypes,
                             'dbname': new_dbname,
                             'distkey': distkey,
                             'sortkey': sortkey}
        create_redshift_table_via_s3(**create_table_args)


def upsert_staging_table_redshift(target_tablename, staging_tablename,
    key_field, dbname='dev', geoserver=False):
    """
    Given a target_tablename, staging_tablename, and key_field,
    uses the staging table to update the target table (using the key field)
    
    Following this method:
    https://docs.aws.amazon.com/redshift/latest/dg/merge-replacing-existing-rows.html  # noqa
    """
    # Delete matching rows from target table
    delete_str = """DELETE FROM {} 
             USING {} as A
             WHERE {}.{} = A.{}; """.format(target_tablename,
                                             staging_tablename,
                                             target_tablename.split('.')[1],
                                             key_field,
                                             key_field)

    # Add updated values from staging table back to target table
    insert_into_str = """INSERT INTO {}
     (SELECT * FROM {});""".format(target_tablename, staging_tablename)

    cmd_list = ['begin transaction;',
                delete_str,
                insert_into_str,
                'end transaction;']

    execute_redshift_cmds(cmd_list,
                          dbname=dbname,
                          geoserver=geoserver)


def post_df_as_redshift_table(df, tablename, schema='basis', dbname='dev',
    dtypes=None, overwrite=True):
    """
    Given a tablename, pandas DataFrame, and optionally a dictionary
    of table columns to sqlalchemy dtypes, writes the DataFrame as a
    Redshift table

    Resource: 
    http://measureallthethin.gs/blog/connect-python-and-pandas-to-redshift/  # noqa

    NOTE: This connection is only used for small data that doesn't need to rely
    on S3. Medium to large data updates take way too long with this method.
    """
    engine = create_redshift_engine(dbname)
    to_Redshift_args = {'name': tablename, 
                        'schema': schema, 
                        'con': engine, 
                        'index': False}
    if overwrite:
        to_Redshift_args['if_exists'] = 'replace'
    else:
        to_Redshift_args['if_exists'] = 'append'
    if dtypes is not None:
        to_Redshift_args['dtype'] = dtypes
    df.to_sql(**to_Redshift_args)


def create_redshift_table_via_s3(tablename, s3_path, ctypes,
    dbname='dev', overwrite=True, distkey=None, sortkey=None,
    diststyle=None, sortstyle=None, gzip=False, header=True, logger=None):
    """
    Notes: 
    - tablename can be of format schema.tablename
    - s3_path is of format s3://{bucket}/{key}

    Inspect table creation errors like this:
    
    SELECT * from stl_load_errors
    WHERE filename LIKE '%<string on s3 path>%'
    ORDER BY starttime DESC;

    Create table Redshift documentation:
    https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_TABLE_NEW.html  # noqa
    """    
    if not overwrite:
        # if table is on Redshift, return
        if check_if_table_exists_on_redshift:
            print_str = 'table exists, not overwriting'
            log_or_print(print_str, logger)
            return

    drop_table_if_exists_cmd = 'DROP TABLE IF EXISTS {}'.format(tablename)

    create_table_str_args = {'tablename': tablename,
                             'ctypes': ctypes,
                             'distkey': distkey,
                             'sortkey': sortkey,
                             'diststyle': diststyle,
                             'sortstyle': sortstyle}

    create_table_cmd = create_redshift_table_str(**create_table_str_args)

    copy_data_args = {'tablename': tablename,
                      's3_path': s3_path,
                      'execute': False,
                      'dbname': dbname,
                      'gzip': gzip,
                      'header': header}
    copy_data_cmd = copy_data_from_s3_to_redshift(**copy_data_args)

    cmds = [drop_table_if_exists_cmd, 
            create_table_cmd,
            copy_data_cmd]

    execute_redshift_cmds(cmds, dbname=dbname)
    print_str = 'table created on Redshift: {}'.format(tablename)
    log_or_print(print_str, logger)
    return create_table_cmd


def copy_data_from_s3_to_redshift(tablename, s3_path, execute=True,
    dbname='dev', gzip=False, header=True):
    """
    s3_path is of format s3://{bucket}/{key}
    """
    set_AWS_creds()
    AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY = get_AWS_creds()

    ignore_header_str = ''
    if header:
        ignore_header_str = '\n                IGNOREHEADER 1'

    gzip_str = ''
    if gzip:
        gzip_str = '\n                GZIP'
        
        
    copy_data_command = """COPY {}
                FROM '{}'
                CREDENTIALS 'aws_access_key_id={};aws_secret_access_key={}'
                EMPTYASNULL
                FILLRECORD
                TIMEFORMAT as 'auto'
                DATEFORMAT as 'auto'
                NULL AS 'nan'
                CSV{}{};""".format(tablename,
                                   s3_path,
                                   AWS_ACCESS_KEY_ID,
                                   AWS_SECRET_ACCESS_KEY,
                                   ignore_header_str,
                                   gzip_str)

    if execute:
        execute_redshift_cmds([copy_data_command], dbname=dbname)

    else:
        return copy_data_command


def unload_data_from_redshift_to_s3(tablename=None, select_q=None, dbname='dev',
    bucket='dataviz-analytics', key_prefix=None, maxfilesize=None, header=True,
    parallel=True, gzip=False, execute=True):
    """
    Given either a tablename or a select_q, unloads the data to S3.

    select_q should be of format 'SELECT {cols} FROM {tables} WHERE ...'.
    If no key_prefix is given and a select_q is specified, will write the
    unloaded table to s3://dataviz-analytics/redshift_table_unloads/query_unload/

    Optionally specify maxfilesize (if int, will default to MB.
    Otherwise specify as string for GB, e.g. '1 GB')

    Redshift UNLOAD docs: 
    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html
    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD_command_examples.html
    """
    if tablename is None and select_q is None:
        print('You must either specify a tablename or a select query')
        return
    set_AWS_creds()
    AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY = get_AWS_creds()

    if select_q is None:
        select_q = 'SELECT * FROM {}'.format(tablename)

    if key_prefix is None:
        if tablename is not None:
            key_prefix = create_s3_key_prefix(tablename, dbname)
        else:
            key_prefix = 'query_unload/'
        # used to be os.path.join but S3 uses forward slash and Windows uses backslash
        key_prefix = '/'.join(['redshift_table_unloads',
                                key_prefix])
        
    unload_q = """UNLOAD ('{}')
    TO 's3://{}/{}'
    CREDENTIALS 'aws_access_key_id={};aws_secret_access_key={}'
    ALLOWOVERWRITE
    CSV""".format(select_q, bucket, key_prefix,
                  AWS_ACCESS_KEY_ID,
                  AWS_SECRET_ACCESS_KEY)

    if maxfilesize is not None:
        unload_q += '\nMAXFILESIZE {}'.format(maxfilesize)
    if not parallel:
        unload_q += '\nPARALLEL OFF'
    if header:
        unload_q += '\nHEADER'
    if gzip:
        unload_q += '\nGZIP'

    if execute:
        execute_redshift_cmds([unload_q], dbname=dbname)
    return key_prefix


def backup_redshift_table(tablename, dbname='dev',
    geoserver=False):
    """
    Given a Redshift tablename, backs up the table to S3
    
    (s3://dataviz-analytics/redshift_table_backups/{dbname}/{schema}/{table}/{backup_timestamp/})
    """    
    schema, table = tablename.split('.')
    backup_timestamp = get_current_time().strftime('%Y-%m-%d_%H:%M')

    prefix = '/'.join(['redshift_table_backups',
                       dbname,
                       schema,
                       table,
                       backup_timestamp,
                       ''])
    
    unload_args = {'tablename': tablename,
                   'dbname': dbname,
                   'key_prefix': prefix}
    
    unload_data_from_redshift_to_s3(**unload_args)


def invoke_lambda(function_name, payload=None, **kwargs):
    """
    Given a lambda function name, and optionally the event payload (dict)
    and additional kwargs (more below), invokes the Lambda function
    
    Boto function docs:
    https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/lambda.html#Lambda.Client.invoke  # noqa
    
    Available kwargs:
    InvocationType='Event'|'RequestResponse'|'DryRun',
    LogType='None'|'Tail',
    ClientContext='string',
    Qualifier='string'
    
    (FunctionName is defined by function_name, Payload is defined by payload)
    """
    import botocore
    import base64
    invoke_lambda_args = {'FunctionName': function_name,
                          'LogType': 'Tail'}
    
    if payload is not None:
        invoke_lambda_args['Payload'] = str.encode(json.dumps(payload))
    client = boto3.client('lambda')
    # from https://github.com/boto/boto3/issues/1104#issuecomment-305136266
    # CLI version: https://github.com/aws/aws-cli/issues/2657#issuecomment-307926335
    func_config = client.get_function_configuration(FunctionName=function_name)
    timeout = func_config['Timeout']
    config = botocore.config.Config(connect_timeout=timeout, read_timeout=timeout)
    client = boto3.client('lambda', config=config)
    response = client.invoke(**{**invoke_lambda_args, **kwargs})
    if 'LogResult' in response:
        print(base64.b64decode(response['LogResult']).decode("utf-8"))
    return response


############################### Socrata functions ###############################

def get_socrata_creds():
    """
    This function sets AWS keys in the environment in order to connect to AWS
    via boto3. This function is mostly used internally by functions that connect
    to AWS. It reads keys from Box: DataViz Projects/Application Secure Files/creds.py
    """
    global SOCRATA_CREDS
    if user in TEAMS['dataviz_team']:
        s_creds = SOCRATA_CREDS
    else:
        other_teams = {k: v for k, v in TEAMS.items() if k != 'dataviz_team'}
        user_team = [k for k, v in other_teams.items() if user in v['members']][0]
        creds_folder = TEAMS[user_team]['folder']
        print('Using {}'.format(creds_folder))
        sys.path.insert(0, '/Users/{}/Box/{}'.format(user, creds_folder))
        from creds import SOCRATA_CREDS
        s_creds = SOCRATA_CREDS
    return s_creds



def create_socrata_client(read_only=False):
    """
    Creates a sodapy Socrata client given credentials
    saved in Application Secure Files Box folder
    """
    # Pre-requisite: ! pip install sodapy
    try:
        from sodapy import Socrata
    except:
        print('need to install sodapy: pip install sodapy')
    s_creds = get_socrata_creds()
    socrata_domain = s_creds['domain']
    # use read-only access creds
    if read_only:
        socrata_user = 'basis'
        print('connecting to Socrata using read-only credentials')
    # connect with user creds
    else:
        socrata_user = user
    try:
        username = s_creds[socrata_user]['username']
        password = s_creds[socrata_user]['password']
        app_token = s_creds[socrata_user]['app_token']
        client = Socrata(socrata_domain, username=username, 
                     app_token=app_token, password=password, timeout=100)
        return client
    except Exception as e:
        print(e)


def get_nrows_from_socrata(socrata_data_id, read_only=False, query_args=None):
    # get number of rows in the data
    num_rows_args = {'select': 'count(*)'}
    if query_args is not None and 'where' in query_args:
        num_rows_args['where'] = query_args['where']
    client = create_socrata_client(read_only)
    num_rows_response = client.get(socrata_data_id, **num_rows_args)
    return int(num_rows_response[0]['count'])


def get_metadata_socrata(socrata_data_id):
    client = create_socrata_client()
    return client.get_metadata(socrata_data_id)


def get_cols_socrata(socrata_data_id):
    metadata = get_metadata_socrata(socrata_data_id)
    return [c['fieldName'] for c in metadata['columns']]


def get_socrata_col_list(metadata=None, socrata_data_id=None):
    if metadata is None and socrata_data_id is None:
        print('Error, must either provide metadata dictionary or socrata 4x4')
        return
    if metadata is None:
        metadata = get_metadata_socrata(socrata_data_id)
        
    columns = [{'fieldName': c['fieldName'],
            'name': c['name'],
            'dataTypeName': c['dataTypeName']} for c in metadata['columns']]
    return columns


def get_row_id_col_socrata(metadata=None, socrata_data_id=None):
    if metadata is None and socrata_data_id is None:
        print('Error, must either provide metadata dictionary or socrata 4x4')
        return
    if metadata is None:
        metadata = get_metadata_socrata(socrata_data_id)
    if metadata.get('rowIdentifierColumnId', None) is not None:
        row_id_colname = [c for c in metadata['columns'] if c['id'] == metadata['rowIdentifierColumnId']][0]['fieldName']
        return row_id_colname


def get_ctypes_socrata(socrata_data_id):
    metadata = get_metadata_socrata(socrata_data_id)
    return {c['fieldName']: c['dataTypeName'] for c in metadata['columns']}


def get_geom_cols_socrata(socrata_data_id):
    """Given a Socrata data id, returns a list of the geometric columns"""
    ctypes = get_ctypes_socrata(socrata_data_id)
    search_strs = ['line', 'point', 'polygon']
    geom_cols = [k for k, v in ctypes.items() if any(s in v for s in search_strs)]
    return geom_cols


def get_redshift_ctype_d_from_socrata(socrata_data_id, varchar_width_multiplier=2):
    ctypes = get_ctypes_socrata(socrata_data_id)
    redshift_ctype_d = {c: 'float' for c in ctypes}
    text_cols = [k for k, v in ctypes.items() if v != 'number']
    df = pull_df_from_socrata(socrata_data_id,
                              query_args={'select': ', '.join(text_cols)})
    for c in text_cols:
        if c in df:  # if there is at least one non-null value
            max_len = int(varchar_width_multiplier
                          * df[c].astype(str).str.len().max()
                          )
        else:  # if all null initialize as varchar(max)
            max_len = 'max'
        redshift_ctype_d[c] = 'varchar({})'.format(max_len)
    return redshift_ctype_d


def add_column_socrata(socrata_data_id, field_name,
                       transform_expr=None, description=None):
    """
    Available transform expressions: https://dev.socrata.com/docs/transforms/
    """
    a = time.time()
    # Pre-requisite: ! pip install socrata-py
    from socrata.authorization import Authorization
    from socrata import Socrata

    socrata_auth_args = {'domain': SOCRATA_CREDS['domain'],
                         'username': SOCRATA_CREDS[user]['username'],
                         'password': SOCRATA_CREDS[user]['password']}

    auth = Authorization(**socrata_auth_args)
    socrata = Socrata(auth)

    # lookup dataset
    dataset_id = socrata_data_id
    (ok, view) = socrata.views.lookup(dataset_id)
    assert ok, view

    (ok, revision) = view.revisions.create_replace_revision()
    assert ok, revision

    (ok, source) = revision.source_from_dataset()
    assert ok, source

    output_schema = source.get_latest_input_schema().get_latest_output_schema()

    # add a new column, https://github.com/socrata/socrata-py#add_column
    add_col_args = {'field_name': field_name, 
                    'display_name': field_name}

    # Default value is null -- 'forgive' transform creates null values
    # https://socratapublishing.docs.apiary.io/#reference/0/inputschema
    if transform_expr is None:
        transform_expr = "forgive(error('hello'))"   

    add_col_args['transform_expr'] = transform_expr
    
    if description is not None:
        add_col_args['description'] = description

    (ok, new_output_schema) = (output_schema
                               .add_column(**add_col_args)
                               .run())

    # Verify that our transforms were successful.
    transform_errors = new_output_schema.any_failed()
    assert(transform_errors==False)

    # Output other errors (if any).
    (ok, response) = new_output_schema.schema_errors()
    if len(response) > 0:
        print(response)
        return

    if transform_errors==False:
        (ok, job) = revision.apply(output_schema = new_output_schema)
        (ok, job) = job.wait_for_finish(progress = lambda job: print('Job progress:', job.attributes['status']))
    b = time.time()
    print('adding column {} took {}'.format(field_name, print_runtime(b-a)))


def pull_df_from_socrata(socrata_data_id, max_chunk_area=1e6, read_only=False,
                         query_args=None, ignore_date_types=False, logger=None):
    """
    Given a data endpoint (e.g. 'fqea-xb6g'), and optionally chunksize, 
    read_only, and query_args (dictionary, can only contain select, 
    where, order, and limit), returns a pandas DataFrame of the Socrata data.

    query_args notes:
    - See here for Socrata query args: https://dev.socrata.com/docs/queries/
    - Args must be in string format, e.g. {'select': 'joinid, fipco'}
    
    Look into: currently, specifiying limit in query_args is causing the
    Socrata data to not return all fields if they are blank for those
    first n rows. For now just don't use this argument.
    """
    a = time.time()
    if query_args is None:
        query_args = {}

    # get number of rows in query:
    #   default is all rows in query if limit is not specified
    if 'limit' in query_args:
        num_rows = query_args['limit']
    #   if limit is not specified get all rows
    else:
        num_rows = get_nrows_from_socrata(socrata_data_id, query_args=query_args)
    if num_rows == 0:
        print_str = 'no rows match your query'
        log_or_print(print_str, logger)
        return

    # set chunk size based on max_chunk_area (default is 1m values):
    #   get number of columns in query
    if 'select' in query_args:
        num_cols = len(query_args['select'].split(', '))
    else:
        col_list = get_cols_socrata(socrata_data_id)
        num_cols = len(col_list)

    # set default chunksize
    chunksize = get_chunksize_socrata((num_rows, num_cols), max_chunk_area)

    query_args['limit'] = chunksize

    # pull data in chunks
    client = create_socrata_client(read_only)
    offset = 0
    num_chunks = math.ceil(num_rows / chunksize)
    print_str = 'pulling data in {} chunks of {} rows each'.format(num_chunks, chunksize)
    log_or_print(print_str, logger)
    all_data = []
    cols = get_cols_socrata(socrata_data_id)
    for i in range(num_chunks):
        print_str = 'pulling chunk {}'.format(i)
        log_or_print(print_str, logger)
        # content_type="csv" can throw a csv_reader field length too long error
        data = client.get(socrata_data_id, offset=offset, **query_args)
        df = pd.DataFrame(data)
        all_data.append(df)
        offset += chunksize
    client.close()
    final_df = pd.concat(all_data, ignore_index=True, sort=False)
    # set correct column types (Socrata returns everything as a string, regardless of actual column type)
    metadata = get_metadata_socrata(socrata_data_id)
    # set data types (since API returns all data as string)
    socrata_ctypes = {c['fieldName']: c['dataTypeName'] for c in metadata['columns']}
    for c in final_df:
        if socrata_ctypes.get(c, 0) == 'number':
            final_df[c] = final_df[c].astype(float)
        if not ignore_date_types:
            if socrata_ctypes.get(c, 0) == 'calendar_date':
                final_df[c] = pd.to_datetime(final_df[c])
    # if not specifying columns to return in 'select', return all cols from dataset
    # (API to json only returns columns that have at least one non-null value)
    if 'select' not in query_args:
        missing_cols = set(cols).difference(set(final_df))
        for c in missing_cols:
            final_df[c] = np.nan
        final_df = final_df[cols]
    b = time.time()
    print_str = 'took {}'.format(print_runtime(b-a))
    log_or_print(print_str, logger)
    return final_df


def pull_geojson_from_socrata(socrata_data_id, read_only=False,
    query_args=None, logger=None):
    """
    Given the socrata_data_id of a Socrata asset with geodata
    (i.e. there must be a column
    with a Socrata geometry datatype)
    """
    a = time.time()
    if query_args is None:
        query_args = {}
    # if limit is not specified get all rows
    if 'limit' not in query_args:
        nrows_args = {'socrata_data_id': socrata_data_id,
                      'read_only': read_only,
                      'query_args': query_args}
        query_args['limit'] = get_nrows_from_socrata(**nrows_args)

    client = create_socrata_client(read_only)
    print_str = 'pulling {} rows as geojson'.format(query_args['limit'])
    log_or_print(print_str, logger)
    geojson = client.get(socrata_data_id, content_type='geojson', **query_args)
    b = time.time()
    print_str = 'took {}'.format(print_runtime(b-a))
    log_or_print(print_str, logger)
    return geojson


def pull_geotable_socrata(socrata_data_id, crs='EPSG:4326', **kwargs):
    import geopandas as gpd
    geom_cols = get_geom_cols_socrata(socrata_data_id)
    if len(geom_cols) < 1:
        print('Not a geo-dataset. Use pull_df_from_socrata instead')
        return
    main_geom_col = geom_cols[0]
    gdf = pull_df_from_socrata(socrata_data_id, **kwargs)
    gdf[main_geom_col] = gdf[main_geom_col].map(to_shapely)
    gdf = gpd.GeoDataFrame(gdf,
                           geometry=main_geom_col,
                           crs=crs)
    return gdf


def create_socrata_dataset(dataset_name, dataset_desc, columns, 
    category, tags, row_identifier=None, logger=None):
    """
    Given a dataset name, dataset description, list of columns (format described below), 
    list of tags (must exist in /admin/metadata), and the name of the
    primary key (row_identifier), creates a dataset on Socrata and returns
    the metadata.

    NOTES:

    - Possible data types at https://dev.socrata.com/docs/datatypes/#

    - Column list must be of format: 
    [{'fieldName': column name, 'name': readable column name , 'dataTypeName': 'text'}, ...]

    - the returned metadata contains the socrata dataset id as 'id'
    """
    client = create_socrata_client()

    socrata_args = {'name': dataset_name, 
                    'description': dataset_desc, 
                    'columns': columns, 
                    'category': category, 
                    'tags': tags}

    if row_identifier is None:
        print_str = 'not setting row identifier'
        log_or_print(print_str, logger)
    else:
        socrata_args['row_identifier'] = row_identifier

    dataset_metadata = client.create(**socrata_args)
    client.close()
    print_str = 'created dataset on Socrata with id: {}'.format(dataset_metadata['id'])
    log_or_print(print_str, logger)
    print_str = 'URL: https://data.bayareametro.gov/d/{}'.format(dataset_metadata['id'])
    log_or_print(print_str, logger)
    return dataset_metadata


def upsert_df_socrata(df, socrata_data_id, chunksize=None,
    load_tmp_json=False, chunks=None, print_messages=True,
    logger=None):
    """
    Given a pandas DataFrame and a Socrata data id, upserts the data to the
    Socrata data set with the given id
    """
    a = time.time()
    ctypes = get_ctypes_socrata(socrata_data_id)
    if load_tmp_json:
        tmp_json_fname = 'upsert_data_socrata.json'
        if os.path.isfile(tmp_json_fname):
            print('loading upsert data from json')
            upsert_data_no_nulls = load_json(tmp_json_fname)
        else:
            
            upsert_data_no_nulls = prepare_df_for_socrata(df,
                ctypes=ctypes,
                print_messages=print_messages)
            dump_json(upsert_data_no_nulls, tmp_json_fname)

    # upsert data in chunks
    if chunksize is None:  #  set default chunksize
        chunksize = get_chunksize_socrata(df.shape)

    num_chunks = math.ceil(len(df) / chunksize)
    if chunks is None:
        chunks = range(num_chunks)
    else:
        num_chunks = len(chunks)
    
    if print_messages:
        print_str = 'upserting data to Socrata in {} chunks of {} rows each'.format(num_chunks,
                                                                                    chunksize)
        log_or_print(print_str, logger)
    for chunk in chunks:
        start_idx = chunk * chunksize
        end_idx = (chunk + 1) * chunksize
        if load_tmp_json:
            upsert_data = upsert_data_no_nulls[start_idx:end_idx]
        else:
            upsert_data = prepare_df_for_socrata(df.iloc[start_idx:end_idx],
                ctypes=ctypes,
                print_messages=False)
        client = create_socrata_client()
        client.upsert(socrata_data_id, upsert_data)
        client.close()
        if print_messages:
            print_str = 'upserted chunk {}'.format(chunk)
            log_or_print(print_str, logger)
    if print_messages:
        print_str = 'data upserted to {}'.format(socrata_data_id)
        log_or_print(print_str, logger)
    if load_tmp_json:
        os.remove(tmp_json_fname)
    b = time.time()
    print_str = 'took {}'.format(print_runtime(b-a))
    log_or_print(print_str, logger)


def publish_socrata_dataset(socrata_data_id, permission_level='private',
    logger=None):
    """
    Publishes dataset to Socrata and sets its audience to the specified
    permission level
    (can be 'private', 'site', public.read', 'public.add')

    NOTE: 'site' corresponds to Socrata Internal audience
    """
    client = create_socrata_client()
    client.publish(socrata_data_id)
    print_str = 'published dataset to {}'.format(socrata_data_id)
    log_or_print(print_str, logger)
    # set dataset permissions (default is private)
    if permission_level != 'private':
        client.set_permission(socrata_data_id, permission_level)
        print_str = 'set dataset permission to {}'.format(permission_level)
        log_or_print(print_str, logger)
    client.close()


def delete_rows_socrata(socrata_data_id, where_str=None, row_ids=None,
    save_backup=True, logger=None):
    """
    Deletes rows from a Socrata dataset based on either filter conditions
    (specified in where_str) or a list of row_ids

    TODO: fix for where_str (for when row_identifier is set)
    
    When a dataset has a row identifier column, deletes rows using that,
    otherwise deletes based using :id column
    (reference: https://dev.socrata.com/docs/system-fields.html)

    Example usage:
    
    # deletes zoning lookup table rows for San Francisco and San Bruno
    delete_where = "city_name in ('San Francisco', 'San Bruno')"
    delete_rows(socrata_data_id='qdrp-c5ra', where_str=delete_where)
    """
    if where_str is not None:
        query_args = {'select': ':*,*',
                      'where': where_str}
        socrata_table = pull_df_from_socrata(socrata_data_id,
            query_args=query_args)
        if save_backup:
            backup_fname = '{}_deleted_rows.csv'.format(socrata_data_id)
            socrata_table.to_csv(backup_fname, index=False)
        row_ids = socrata_table[':id'].unique()
    else:
        if row_ids is None:
            print_str = 'need to provide either a filter condition (where_str) or a list of row_ids to delete'
            log_or_print(print_str, logger)
    
    print_str = 'deleting {} rows'.format(len(row_ids))
    log_or_print(print_str, logger)
    client = create_socrata_client()
    for row_id in row_ids:
        try:
            client.delete(socrata_data_id, row_id=row_id)
        except Exception as e:
            print_str = 'failed to delete row {} \n {}'.format(row_id, e)
            log_or_print(print_str, logger)
    log_or_print('deleted', logger)


def delete_socrata_asset(socrata_data_id, logger=None):
    """
    Given a socrata_data_id, deletes the asset from Socrata.

    Formerly called delete_df_socrata
    """
    client = create_socrata_client()
    client.delete(socrata_data_id)
    print_str = 'asset deleted from Socrata: {}'.format(socrata_data_id)
    log_or_print(print_str, logger)


def replace_df_socrata(df, socrata_data_id, logger=None):
    """
    Given a new dataframe and socrata dataset 4x4,
    replaces that dataset with the new dataframe
    """
    ctypes = get_ctypes_socrata(socrata_data_id)
    data = prepare_df_for_socrata(df, ctypes=ctypes)
    client = create_socrata_client()
    metadata = client.replace(socrata_data_id, data)
    log_or_print(repr(metadata), logger)


def create_dummy_socrata_dataset(df=None):
    """
    Creates and returns the Socrata dataset 4x4
    of a dummy dataset
    """
    dataset_name = 'test_dataset'
    dataset_desc = 'Dummy Socrata dataset for testing'
    if df is None:
        df = pd.DataFrame({'a': ['a', 'b', 'c'],
                           'b': [1, 2, 3],
                           'c': [1.0, 2.0, 3.0],
                           'd': [True, False, True]
                          })
    df['recid'] = generate_uuid_list(len(df))
    ctype_d = create_column_type_dict(df)
    ctype_d = redshift_to_socrata_ctype_d(ctype_d)
    columns = create_socrata_column_list(ctype_d)
    category = 'Development'
    tags = ['mtc']
    row_identifier = 'recid'
    metadata = create_socrata_dataset(dataset_name, dataset_desc, columns, 
    category, tags, row_identifier)
    socrata_data_id = metadata['id']
    upsert_df_socrata(df, socrata_data_id)
    publish_socrata_dataset(socrata_data_id)
    return socrata_data_id


def backup_socrata_dataset(socrata_data_id, s3_prefix=None,
    only_if_change_exists=False):
    """
    Given a Socrata dataset 4x4, backs up the dataset to S3.
    If only_if_change_exists is True, will only back up the
    dataset if it is not already on S3 or if it has changed
    from the most recent backup.
    """
    metadata = get_metadata_socrata(socrata_data_id)

    s3_bucket = 'dataviz-analytics'
    if s3_prefix is None:
        s3_prefix = os.path.join('socrata_table_backups',
                                 metadata['category'],
                                 metadata['name'])

    ts = datetime.fromtimestamp(metadata['rowsUpdatedAt']).isoformat()
    backup_ts = datetime.now().isoformat().split('.')[0]
    s3_key_base = os.path.join(s3_prefix, '{}_{}'.format(socrata_data_id, ts))
    s3_key = '{}_backup_{}.csv'.format(s3_key_base, backup_ts)
    # if only backing up if there is a change
    if only_if_change_exists:
        # if backups of this dataset already exists, only back up if change exists
        keys = list_s3_keys(s3_bucket, Prefix=s3_key_base)
        if len(keys) > 0:
            most_recent = pd.Series(keys).max()
            most_recent_backup = pull_df_from_s3(s3_bucket, most_recent)
            # since when pulling from S3, date types convert to string
            df = pull_df_from_socrata(socrata_data_id, ignore_date_types=True)
            old_vals = find_df_difference(df, most_recent_backup, return_new=False)
            if len(old_vals) > 0:  # if change exists
                post_df_to_s3(df, s3_bucket, s3_key)

    else:
        # since when pulling from S3, date types convert to string
        df = pull_df_from_socrata(socrata_data_id, ignore_date_types=True)
        post_df_to_s3(df, s3_bucket, s3_key)
    return 's3://{}/{}'.format(s3_bucket, s3_key)


def create_socrata_dataset_copy(socrata_data_id, copy_data=True):
    """
    Given a 4x4, creates a copy of the dataset and returns the new 4x4.
    If empty is False, only copies the metadata (and skips the data copy).

    NOTE: for large datasets, set copy_data=False, use upsert_df_socrata()
    and play with the chunksize parameter (since the default chunksize
    may be too large and fail in this function)
    """
    # get metadata
    metadata = get_metadata_socrata(socrata_data_id)

    columns = get_socrata_col_list(metadata)
    row_identifier = get_row_id_col_socrata(metadata)

    create_socrata_dataset_args = {'dataset_name': metadata['name'],
                                   'dataset_desc': metadata.get('description'),
                                   'category': metadata.get('category'),
                                   'tags': metadata.get('tags'),
                                   'columns': columns,
                                   'row_identifier': row_identifier}

    # creates a private dataset
    dataset_metadata = create_socrata_dataset(**create_socrata_dataset_args)
    new_data_id = dataset_metadata['id']
    if copy_data:
        df = pull_df_from_socrata(socrata_data_id)
        upsert_df_socrata(df, new_data_id)
    return new_data_id


def create_socrata_dataset_from_view(socrata_data_id, delete_view=False,
                                     permission_level='private'):
    """
    Given the 4x4 of a View on Socrata, creates a Dataset on Socrata
    with its metadata/data and the given permission_level
    (see publish_socrata_dataset for options).
    
    If delete_view is True, deletes the View once the Dataset has been created.
    """
    socrata_link_base = 'https://data.bayareametro.gov/d/{}'

    if delete_view:
        view_link_str = ''
    else:
        # if we are not deleting View, print its link in the dataset description
        view_link_str = ('\nOriginal view link: {} \n'
                         .format(socrata_link_base
                                 .format(socrata_data_id)
                                )
                        )

    # get View metadata
    metadata = get_metadata_socrata(socrata_data_id)

    parent_dataset_link = socrata_link_base.format(metadata['modifyingViewUid'])

    view_desc = """This dataset was created on {} from a view based on {}\n
    View was created on {} by {} ({})\n {}
    Query used to generate view: \n {}""".format(get_current_time().strftime('%B %d, %Y'),
                                                 parent_dataset_link,
                                                 (pd.to_datetime(metadata['createdAt'], unit='s')
                                                  .strftime('%B %d, %Y')),
                                                 metadata['owner']['displayName'],
                                                 metadata['privateMetadata']['contactEmail'],
                                                 view_link_str,
                                                 metadata['queryString'])

    columns = [{'fieldName': c['fieldName'],
                'name': c['name'],
                'dataTypeName': c['dataTypeName']} for c in metadata['columns']]

    create_dataset_args = {'dataset_name': metadata['name'],
                           'dataset_desc': view_desc,
                           'columns': columns, 
                           'category': metadata['category'],
                           'tags': metadata['tags']}

    # create Dataset on Socrata
    metadata = create_socrata_dataset(**create_dataset_args)
    new_socrata_data_id = metadata['id']
    
    # pull data from View
    print('pulling data from View')
    df = pull_df_from_socrata(socrata_data_id)
    
    # push data to Dataset
    print('pushing data to Dataset')
    upsert_df_socrata(df, new_socrata_data_id)
    
    publish_socrata_dataset(new_socrata_data_id, permission_level=permission_level)
    
    if delete_view:
        delete_socrata_asset(socrata_data_id)


def copy_table_from_redshift_to_socrata(redshift_table_name, select_q=None,
    chunksize=100000, socrata_data_id=None, create_table_kwargs=None,
    replace=True, df_transform_f=None, logger=None):
    """
    Given a table on Redshift, either creates, replaces, or appends to a table on Socrata

    Steps:
    - get redshift metadata -> init socrata dataset
         (if socrata_data_id is passed, verify that fields match)
    - unload redshift dataset to S3
    - by chunk, pull from S3 and push to Socrata
    - delete unloaded keys from S3


    create_table_kwargs must be of format

    create_table_kwargs =  {'dataset_name': str,
                            'dataset_desc': str,
                            'category': str,
                            'tags': list of str,
                            'row_identifier': str (optional),
                            'permission_level': 'site', default is private}
    """
    # pull Redshift table metadata
    ctype_d = get_ctypes_redshift(redshift_table_name)
    ctype_d = redshift_to_socrata_ctype_d(ctype_d)
    # copy to existing Socrata table
    if socrata_data_id is not None:
        if not replace:
            socrata_ctypes = get_ctypes_socrata(socrata_data_id)
            if socrata_ctypes != ctype_d:
                print_str = 'column schemas do not match, cannot copy data from Redshift to Socrata'
                log_or_print(print_str, logger)
                return
    # create new Socrata table
    else:
        columns = create_socrata_column_list(ctype_d)
        if 'permission_level' in create_table_kwargs:
            permission_level = create_table_kwargs.pop('permission_level')
        create_table_kwargs['columns'] = columns
        metadata = create_socrata_dataset(**create_table_kwargs)
        socrata_data_id = metadata['id']
        
    # unload Redshift dataset to S3, upsert to Socrata in chunks
    bucket = 'dataviz-analytics'
    unload_args = {'bucket': bucket,
                   'tablename': redshift_table_name,
                   'select_q': select_q
                   }
    key_prefix = unload_data_from_redshift_to_s3(**unload_args)

    # get list of filenames on S3
    s3_files = list_s3_keys(bucket, Prefix=key_prefix)
    for idx, f in enumerate(s3_files):
        try:
            df = pull_df_from_s3(bucket, f)
            # df transform
            if df_transform_f is not None:
                df_transform_f(df)
            n_chunks = math.ceil(len(df)/chunksize)
            df_chunks = np.array_split(df, n_chunks)
            print_str = 'processing chunk {} into {} sub-chunks'.format(idx, n_chunks)
            log_or_print(print_str, logger)
            for df_chunk_idx, df_chunk in enumerate(df_chunks):
                if idx == 0 and df_chunk_idx == 0 and replace:
                    metadata = replace_df_socrata(df_chunk.iloc[:100], socrata_data_id)
                    print(metadata)
                    upsert_df_socrata(df_chunk.iloc[100:], socrata_data_id,
                        print_messages=False)
                else:
                    upsert_df_socrata(df_chunk, socrata_data_id,
                        print_messages=False)
                print_str = 'upserted sub-chunk {} of chunk {}\n'.format(df_chunk_idx,
                                                                         idx)
                log_or_print(print_str, logger)
        except Exception as e:
            log_or_print(e, logger)

    # clean up S3 folder
    delete_s3_keys(bucket, key_prefix)

    # publish new dataset
    try:
        publish_socrata_dataset(socrata_data_id)
    except:
        pass
    print_str = 'copy from Redshift to Socrata complete'
    log_or_print(print_str, logger)


def copy_table_from_socrata_to_redshift(socrata_data_id, tablename, dbname='dev',
    logger=None):
    """
    Copy table from Socrata to Redshift
    - get column types
    - backup socrata dataset to S3 (in redshift_socrata_transfers/{dbname}/{schema}/{table}/Socrata_Tablename.csv
    - create from S3
    """
    # get column types
    print_str = 'getting column types:\n'
    log_or_print(print_str, logger)
    redshift_ctype_d = get_redshift_ctype_d_from_socrata(socrata_data_id)
    
    # backup socrata dataset to S3
    print_str = '\nbacking up to S3:\n'
    log_or_print(print_str, logger)
    if '.' not in tablename:
        # if schema is not specified, it is public
        schema, table = 'public', tablename
    else:
        schema, table = tablename.split('.')

    metadata = get_metadata_socrata(socrata_data_id)
    s3_prefix = os.path.join('redshift_socrata_transfers',
                             dbname,
                             schema,
                             table,
                             '{}_{}'.format(metadata['name'].replace(' ', '_'),
                                            socrata_data_id)
                            )
    s3_path = backup_socrata_dataset(socrata_data_id, s3_prefix=s3_prefix)
    
    # create Redshift table from S3
    print_str = '\ncreating on Redshift:\n'
    log_or_print(print_str, logger)
    create_table_args = {'tablename': tablename,
                         's3_path': s3_path,
                         'ctypes': redshift_ctype_d,
                         'dbname': dbname}
    create_redshift_table_via_s3(**create_table_args)


def get_epsg_proj(spatialReference):
    # Pre-requisite: ! pip install beautifulsoup4
    from bs4 import BeautifulSoup
    if 'wkt' in spatialReference:
        geo_wkt = spatialReference['wkt']
        search_str = urllib.parse.quote(geo_wkt.split(',')[0])
        data = requests.get('https://www.spatialreference.org/ref/?search={}'.format(search_str))
        soup = BeautifulSoup(data.text)
        epsg = int(soup.find('li').find('a').text.split(':')[1])
        print('using epsg {} \n (based on {})'.format(epsg, geo_wkt))
    elif 'wkid' in spatialReference:
        epsg = spatialReference['wkid']
    return epsg


def create_arcgis_client(anon=False, admin=False):
    """
    This function authenticates using a client id.
    This method opens a window and requires manually signing in
    according to the process below

    Sign in steps:
    - choose Enterprise login
    - enter mtc in login url box
    - copy OAuth2 code that pops up in new window
    - enter code into the prompt/notebook cell where you are connecting

    Create client id:
    https://developers.arcgis.com/python/guide/working-with-different-authentication-schemes/#User-authentication-with-OAuth-2.0
    """
    # Pre-requisite: ! pip install arcgis
    from arcgis.gis import GIS
    if anon:
        gis = GIS()
        print('Anonymous ArcGIS client created')
    elif admin:
            gis_args = {'url': 'https://www.arcgis.com',
                        'username': ARCGIS_CREDS['admin']['username'],
                        'password': ARCGIS_CREDS['admin']['password']}
            gis = GIS(**gis_args)
            print('Admin ArcGIS client created')
    else:  # use user-specific client ID
        client_id = ARCGIS_CREDS[user]
        # this part opens a window and requires manually signing in and copying the OAuth2 code and entering here
        gis = GIS(client_id=client_id)
        print('ArcGIS client created')
    return gis


def pull_geodata_from_argis(arcgis_data_id, geo_colname='SHAPE', client=None, **kwargs):
    """
    Given the ArcGIS data id and column name of the geometry field,
    returns a geopandas GeodDataFrame


    e.g. given ArcGIS resource with this web address:
    
    https://mtc.maps.arcgis.com/home/item.html?id=d97b4f72543a40b2b85d59ac085e01a0,
    
    run 

    gdf = get_geodata_from_argis('d97b4f72543a40b2b85d59ac085e01a0')


    Fixed SDF problem by upgrading to SEDF:
    https://community.esri.com/thread/228704-nameerror-oldpandas-spatial-data-fram-from-featurelayer  # noqa

    (was previously sdf = SpatialDataFrame.from_layer(layer) but that is now officially deprecated)

    SEDF docs:
    https://developers.arcgis.com/python/guide/introduction-to-the-spatially-enabled-dataframe/
    """
    import geopandas as gpd
    # Pre-requisite: ! pip install arcgis

    if client is None:
        gis = create_arcgis_client(**kwargs)
    else:
        gis = client
    data_item = gis.content.get(arcgis_data_id)
    layer = data_item.layers[0]
    spatialReference = layer.properties['extent']['spatialReference']
    epsg = get_epsg_proj(spatialReference)
    sdf = pd.DataFrame.spatial.from_layer(layer)
    sdf_cols = [c for c in list(sdf) if c != geo_colname]
    sdf['wkt'] = sdf[geo_colname].map(get_wkt)
    sdf['geom'] = sdf['wkt'].map(to_shapely)
    gdf = gpd.GeoDataFrame(sdf[sdf_cols + ['geom']], geometry='geom')
    # gdf.crs = {'init': 'epsg:{}'.format(epsg)}
    # Fixing `pyproj FutureWarning: '+init=<authority>:<code>' syntax is deprecated`
    # Accepted formats: https://geopandas.org/projections.html
    gdf.crs = 'EPSG:{}'.format(epsg)
    return gdf


def pull_tabledata_from_arcgis(arcgis_data_id, client=None):
    """
    Given an AGOL data table id (found at the end of the asset URL),
    returns the data as a pandas DataFrame
    """
    if client is None:
        gis = create_arcgis_client()
    else:
        gis = client
    data_item = gis.content.get(arcgis_data_id)
    table = data_item.tables[0]
    qd = table.query().to_dict()
    df = pd.DataFrame([f['attributes'] for f in qd['features']])
    return df


def get_downloadable_content_arcgis(arcgis_data_id, admin=True,
                                    geodatabase=True, keep_downloads=True):
    
    import fiona
    import geopandas as gpd
    from zipfile import ZipFile

    client = create_arcgis_client(admin=admin)
    # use downloadable content id
    item = client.content.get(arcgis_data_id)
    # download content
    zip_path = item.download('.')
    zip_file = ZipFile(zip_path)
    zip_file.extractall(path='.')
    if geodatabase:
        output_fname = glob.glob('*.gdb')[0]
        layers = fiona.listlayers(output_fname)
        gdf = gpd.read_file(output_fname, layer=layers[0])
        if not keep_downloads:
            delete_file(zip_path)
            delete_dir(output_fname)
        return gdf
    else:
        print('Non-geodatabase assets not yet supported')


def geojson_to_gpkg(geojson_src_fname, gpkg_dest_fname, layer_name, crs=None,
    logger=None):
    """
    Converts a geojson file to a geopackage for ArcGIS.
    
    Needs to run in geo_env (or any geopandas-enabled environment)
    """
    import geopandas as gpd
    geojson = load_json(geojson_src_fname)
    gdf = gpd.GeoDataFrame.from_features(geojson)
    gdf.crs = crs
    gdf.to_file(gpkg_dest_fname, layer=layer_name, driver='GPKG')
    print_str = 'converted {} to geopackage: {}'.format(geojson_src_fname, gpkg_dest_fname)
    log_or_print(print_str, logger)


def get_http_response(url, auth_key=None):
    request_args = {'url': url}
    if auth_key is not None:
        request_args['headers'] = {'Authorization': auth_key}
    return requests.get(**request_args)


def pull_bay_counties():
    """
    Returns a list of 9 Bay Area counties
    
    Pulls county names from
    [County Cities](https://data.bayareametro.gov/Equivalencies/County-Cities/bcnj-9hqr)  # noqa
    on Socrata
    """
    bay_cty = pull_df_from_socrata('bcnj-9hqr',
                                      query_args={'select': 'county_name'})
    bay_cty = list(bay_cty['county_name'].unique())
    return bay_cty


def pull_bay_cities():
    """
    Returns a list of 109 Bay Area cities
    
    Pulls city names from
    [County Cities](https://data.bayareametro.gov/Equivalencies/County-Cities/bcnj-9hqr)  # noqa
    on Socrata
    """
    bay_cities = pull_df_from_socrata('bcnj-9hqr',
                                      query_args={'select': 'city_name'})
    bay_cities = list(bay_cities['city_name'].unique())
    return bay_cities


def pull_bay_cities_counties():
    """
    Returns a list of 109 Bay Area cities + counties
    
    Pulls from
    [County Cities](https://data.bayareametro.gov/Equivalencies/County-Cities/bcnj-9hqr)  # noqa
    on Socrata
    """
    pull_cols = ['city_name', 'county_name']
    bay_city_cty = pull_df_from_socrata('bcnj-9hqr',
                                        query_args={'select': ', '.join(pull_cols)})
    bay_city_cty = bay_city_cty.drop_duplicates()
    return bay_city_cty


def pull_bay_cities_geo():
    """
    Pull cities from OpenData:
    https://opendata.mtc.ca.gov/datasets/san-francisco-bay-region-jurisdictions-incorporated-places-and-unincorporated-county-lands  #noqa
    """
    import geopandas as gpd
    opendata_url_base = 'https://opendata.arcgis.com/datasets/'
    city_url = opendata_url_base + '4b1242e5cb224a2c9043927d3344df5a_0.geojson'
    cities = gpd.read_file(city_url)
    # Cast as multipolygon
    cities['multipoly'] = cities['geometry'].map(to_multipoly)
    cities = cities.set_geometry('multipoly')
    cities = (cities
              .drop('geometry', axis=1)
              .rename(columns={'multipoly': 'geometry'})
             )

    # Standardize to MTC set of 109 cities: 'St Helena'
    q = "jurname == 'Saint Helena'"
    cities.at[cities.eval(q), 'jurname'] = 'St Helena'

    # Add county name:
    # https://data.bayareametro.gov/Equivalencies/County-Cities/bcnj-9hqr  # noqa
    counties = pull_df_from_socrata('bcnj-9hqr',
                                    query_args={'select': 'fipco, county_name'}
                                   )
    counties = counties.drop_duplicates()
    counties['fipco'] = (counties['fipco'].map(float_to_intstr).str.zfill(5))
    counties = dict(zip(counties['fipco'], counties['county_name']))

    cities['fipco'] = cities['fipst'] + cities['fipco']
    cities['county'] = cities['fipco'].map(counties)

    keep_cols = ['county', 'jurname', 'geometry']
    cities = gpd.GeoDataFrame(cities[keep_cols],
                              geometry='geometry',
                              crs='EPSG:4326')
    return cities


def pull_bay_counties_geo():
    """
    Pull counties from OpenData:
    https://opendata.mtc.ca.gov/datasets/-san-francisco-bay-region-counties  
    """
    import geopandas as gpd
    opendata_url_base = 'https://opendata.arcgis.com/datasets/'
    cty_url = opendata_url_base + '1241065779a149de9784c3050d77d3e1_4.geojson'
    counties = gpd.read_file(cty_url)
    # Exclude Farallon Islands
    counties.at[1, 'geometry'] = list(counties.at[1, 'geometry'])[0]
    # Clean up dataframe
    counties['fipco_str'] = 'CA' + counties['fipco']
    counties = counties[['fipco_str', 'coname', 'geometry']]
    counties['coname'] = counties['coname'].str.replace(' County', '')
    counties = counties.rename(columns={'coname': 'county_name'})
    return counties


def pull_acs_geodata(year=2018, geo_type='Tracts_Blocks'):
    """
    We are using Census Blocks as Blocks nest within tracts.
    We can therefore get Tracts from Blocks by splitting out the tract
    geoid from the block geoid.


    Author: Joshua Croff
    Source:
    [Census TIGERweb REST API Documentation](https://tigerweb.geo.census.gov/tigerwebmain/TIGERweb_restmapservice.html)  # noqa
    """
    import geopandas as gpd
    if year != 2018 or geo_type != 'Tracts_Blocks':
        print('Error- not yet supported. Use default arguments only.')
        return
    # Create block group geodataframe from geojson data pulled from Census API
    state = '06'
    counties = "('001','013','041','055','075','081','085','095','097')"
    where_str = "where=STATE='{state}'\
    +AND+COUNTY+IN{counties}".format(state=state,
                                    counties=counties)
    query_args = [where_str, 'outFields=GEOID,BLKGRP&f=geojson']
    url = '/'.join(['https://tigerweb.geo.census.gov',
                    'arcgis',
                    'rest',
                    'services',
                    'Generalized_ACS2018',
                    'Tracts_Blocks',
                    'MapServer',
                    '4',
                    'query?{}'.format('&'.join(query_args))])
    r = get_http_response(url)
    blkgp_2018 = r.json()
    blkgp_2018 = gpd.GeoDataFrame.from_features(blkgp_2018['features'],
                                                crs='EPSG:4326')
    # Create a tract column using string slicing
    # #  trct_geoid is all but last digit of blkg_geoid (GEOID)
    blkgp_2018['trct_geoid'] = blkgp_2018['GEOID'].str[:-1]

    # Rename GEOID column to blkg_geoid (block group geoid)
    blkgp_2018 = (blkgp_2018
        .rename(columns={'GEOID':'blkg_geoid',
                         'BLKGRP': 'BLKGRP'.lower()})
        )
    final_col_order = ['blkg_geoid',
                       'trct_geoid',
                       'blkgrp',
                       'geometry']
    blkgp_2018 = blkgp_2018[final_col_order]
    return blkgp_2018


def pull_realtime_vehicle_positions_from_swiftly(agencykey, write_to_json=True):
    """
    Given an agencykey, pulls realtime GTFS data for that agency
    and returns the data as a dictionary, optionally writing it
    as realtime_vehicle_positions/{agencykey}_rtGTFS_{timestamp}.json
    
    List of valid agencykeys: 'actransit', 'actransit-mtc', 'smart', 'vta', 'vta-test', 'napa-valley'
    """
    from google.protobuf import json_format
    from google.transit import gtfs_realtime_pb2

    key = SWIFTLY_CREDS[user]
    url = 'https://api.goswift.ly/real-time/{}/gtfs-rt-vehicle-positions'.format(agencykey)
    response = requests.get(url, headers={'Authorization': key})
    
    # parse Protocol Buffer
    feed = gtfs_realtime_pb2.FeedMessage()
    feed.ParseFromString(response.content)
    json_string = json_format.MessageToJson(feed)
    msg_json = json.loads(json_string)
    
    timestamp = msg_json['header']['timestamp']
    vehicle_positions = {'timestamp': timestamp,
                     'updates': []}
    for update in msg_json['entity']:
        vehicle_positions['updates'].append(flatten_vehicle_position_update_d(update))
    
    if write_to_json:
        output_dir = os.path.join('realtime_vehicle_positions', agencykey)
        makedirs_if_not_exists(output_dir)
        output_fname = os.path.join(output_dir,
            '{}_rtGTFS_{}.json'.format(agencykey, timestamp))
        dump_json(vehicle_positions, output_fname)
    return vehicle_positions


def google_geocode(address, return_coords_only=True, print_messages=False):
    """
    Given a single address, if return_coords_only is True, returns the (x, y)
    (long, lat) coordinates for the geocoded address (if location_type == 'ROOFTOP').
    If return_coords_only is False, returns the full JSON geocoder response.
    """
    # G_KEY = G_CREDS[user]
    if user in TEAMS['dataviz_team']:
        G_KEY = G_CREDS['dataviz_team']
    else:
        print('Please contact the DataViz team for a Google geocoding API key')
        return
    try:
        coords = (None, None)
        addr_formatted = urllib.parse.quote_plus(address)
        url = 'https://maps.googleapis.com/maps/api/geocode/json?address={}&key={}'.format(addr_formatted,
                                                                                           G_KEY)
        resp = requests.get(url)
        resp_json = json.loads(resp.text)
        match = resp_json['results'][0]['geometry']
        location_type = match['location_type']
        if print_messages:
            print('location type: ', location_type)
        # now only adding geocoded coords if result if ROOFTOP
        if location_type == 'ROOFTOP':
            coords = (match['location']['lng'], match['location']['lat'])
        if return_coords_only:
            return coords
        else:
            return resp_json['results']
    except:
        return


def google_geocode_batch(address_list, include_details=False):
    res = {}
    for address in address_list:
        # coords if return_coords_only=True, otherwise geocoder API json response
        geocoded_addr = google_geocode(address, return_coords_only)
        if geocoded_addr is None:
            continue
        # if return_coords_only:
        #     res[address] = geocoded_addr
        #     return res
        else:
            flattened_resp_d = flatten_nested_dict(geocoded_addr[0])
            if include_details:
                addr_comps = flattened_resp_d['address_components']
                flat_addr_d = {}
                for item in addr_comps:
                    new_k = '_'.join(item['types'])
                    flat_addr_d[new_k + '_long_name'] = item['long_name']
                    flat_addr_d[new_k + '_short_name'] = item['short_name']

                final_flattened_d = {**{k: v for k, v in flattened_resp_d.items() if k != 'address_components'},
                                     **flat_addr_d}

                final_flattened_d['types'] = ', '.join(final_flattened_d['types'])

                resp_df = pd.DataFrame([final_flattened_d])
            else:
                subset_cols = ['address_orig', 'geometry', 'geometry_location_type']
                include_keys = subset_cols + ['geometry_location_lng', 'geometry_location_lat']
                subset_d = {k: v for k, v in flattened_resp_d.items() if k in include_keys}
                resp_df = pd.DataFrame([subset_d])
            resp_df['address_orig'] = [address]
            res[address] = resp_df
    # if not return_coords_only:
    #     res = pd.concat(res.values(), ignore_index=True, sort=False)

    res = gpd.GeoDataFrame(res,
                             geometry=gpd.points_from_xy(res['geometry_location_lng'],
                                                         res['geometry_location_lat']))
    if not include_details:
        res = res[subset_cols]
    return res


def mapquest_geocode(address_list):
    """
    Given a list of addresses, batch geocodes up to 100
    using the free MapQuest geocoder. Returns two dictionaries for
    address latitudes and longitudes
    
    API docs: https://developer.mapquest.com/documentation/geocoding-api/

    Mapquest Developer API allows 15k transactions per month:
    https://developer.mapquest.com/plans

    Function was previously called geocode_addresses_mapquest
    """
    address_search_params = {'locations': [],
                             'options':{'thumbMaps': False,
                                        'maxResults':'1'}
                            }
    for addr in address_list[:100]:
        address_search_params['locations'].append({'street': addr})

    geocode_url_base = 'https://www.mapquestapi.com/geocoding/v1/batch'
    urlencode_args = {'inFormat': 'json',
                      'outFormat': 'json',
                      'json': json.dumps(address_search_params),
                      'key': MAPQUEST_CREDS[user]['key']}
    # formulate MapQuest geocoding API call string
    geocode_request_args = urllib.parse.urlencode(urlencode_args,
                                                  safe=':,[]{}" ',
                                                  quote_via=urllib.parse.quote)
    geocode_url = '{}?{}'.format(geocode_url_base, geocode_request_args)

    # make MapQuest geocoding API call
    resp = requests.get(geocode_url)

    # parse results
    response_json = resp.json()
    location_results = response_json['results']

    geocode_lat_d = {}
    geocode_lng_d = {}
    for r in location_results:
        lat_long = r['locations'][0]['latLng']
        addr = r['providedLocation']['street']
        geocode_lat_d[addr] = lat_long['lat']
        geocode_lng_d[addr] = lat_long['lng']
    return geocode_lat_d, geocode_lng_d


def get_511_creds():
    return DEV_511_CREDS[user]
